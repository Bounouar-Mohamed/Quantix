/**
 * Serveur WebSocket pour Realtime AI
 * G√®re le proxy vers OpenAI Realtime API
 */

import WebSocket from 'ws';
import { Server } from 'http';
import OpenAI, { toFile } from 'openai';
import { defaultProfile } from '../../../ai/modelProfile';
import { buildRealtimeSessionUpdate } from '../ai/transports/openaiRealtime';
import { executeTool } from '../ai/toolRegistry';
import { ChatService } from '../ai/services/chatService';
import { generateTTS } from '../ai/services/ttsService';
import { 
    forwardAssistantText, 
    forwardAssistantAudioDelta, 
    forwardAssistantCitations, 
    forwardUserTranscript 
} from '../ai/eventRouter';

/**
 * Interface pour une connexion Realtime
 */
interface RealtimeConnection {
    userId: string;
    threadId: string;
    conversationId?: string; // Pour journalisation dans thread
    tenantId?: string; // Pour isolation multi-tenant
    assistantThreadId?: string; // ID thread OpenAI
    openaiWs: WebSocket;
    connectedAt: Date;
}

interface UserSTTState {
    pcmChunks: Buffer[];
    sampleRate: number;
    hasReceivedTranscription: boolean;
    lastTranscript?: string; // Pour le fallback REST
    accumulatedTranscript?: string; // Accumulateur de deltas
    responseGenerationStarted?: boolean; // Flag pour √©viter double g√©n√©ration
}


/**
 * Serveur WebSocket pour Realtime
 */
export class RealtimeWebSocketServer {
    private wss: WebSocket.Server;
    private connections = new Map<WebSocket, RealtimeConnection>();
    private userSTTStates = new Map<string, UserSTTState>();
    private openai: OpenAI;
    private chatService: ChatService;

    constructor(server: Server) {
        this.wss = new WebSocket.Server({
            server,
            path: process.env.REALTIME_WS_PATH || '/realtime',
        });

        // Initialiser OpenAI
        if (!process.env.OPENAI_API_KEY) {
            throw new Error('OPENAI_API_KEY manquante');
        }

        this.openai = new OpenAI({
            apiKey: process.env.OPENAI_API_KEY,
        });

        // Initialiser le service Chat
        this.chatService = new ChatService(this.openai);

        this.setupEventHandlers();

        console.log('‚úÖ Serveur WebSocket Realtime initialis√© sur /realtime');
        
        // Log √©tat de la recherche web
        const webSearchEnabled = process.env.WEB_SEARCH_ENABLED === 'true';
        const hasGoogleApiKey = !!process.env.GOOGLE_CUSTOM_SEARCH_API_KEY;
        const hasGoogleEngineId = !!process.env.GOOGLE_CUSTOM_SEARCH_ENGINE_ID;
        
        if (webSearchEnabled) {
            console.log('üîç Recherche web: ACTIV√âE');
            if (hasGoogleApiKey && hasGoogleEngineId) {
                console.log('   ‚úÖ Google Custom Search configur√©');
            } else {
                console.warn('   ‚ö†Ô∏è GOOGLE_CUSTOM_SEARCH_API_KEY ou GOOGLE_CUSTOM_SEARCH_ENGINE_ID manquants');
            }
        } else {
            console.log('üîç Recherche web: D√âSACTIV√âE');
        }
    }

    /**
     * Configuration des handlers
     */
    private setupEventHandlers() {
        this.wss.on('connection', (clientWs: WebSocket, req) => {
            this.handleConnection(clientWs, req);
        });
    }

    /**
     * NOTE: La journalisation des messages Realtime dans le thread est maintenant g√©r√©e par le FRONT.
     * 
     * Le front se connecte directement √† OpenAI Realtime (WebRTC) donc le serveur ne voit pas les √©v√©nements.
     * Le front doit POSTer vers /api/v1/assistants/thread/messages pour chaque √©v√©nement :
     * - input_transcription.completed ‚Üí role: 'user'
     * - response.completed ‚Üí role: 'assistant'
     * 
     * Voir FRONT_REALTIME_INTEGRATION.md pour les d√©tails d'int√©gration front.
     * 
     * Cette m√©thode n'est plus utilis√©e mais conserv√©e pour r√©f√©rence.
     */
    private async journalizeMessage(
        conversationId: string,
        role: 'user' | 'assistant',
        content: string,
        tenantId?: string
    ): Promise<void> {
        // D√©sactiv√© : le front journalise directement via POST /assistants/thread/messages
        console.warn(`‚ö†Ô∏è journalizeMessage appel√©e mais d√©sactiv√©e - le front doit journaliser directement`);
    }

    /**
     * G√©rer une nouvelle connexion
     */
    private async handleConnection(clientWs: WebSocket, req: any) {
        try {
            // Extraire les param√®tres depuis l'URL
            const url = new URL(req.url, `http://${req.headers.host}`);
            const threadId = url.searchParams.get('threadId');
            const userId =
                url.searchParams.get('userId') ??
                url.searchParams.get('conversationId') ??
                undefined;
            const conversationId = url.searchParams.get('conversationId') || userId;
            const assistantThreadId = url.searchParams.get('assistant_thread_id') || undefined;
            const tenantId = url.searchParams.get('tenantId') || undefined;

            if (!threadId || !userId) {
                clientWs.close(1008, 'threadId et userId requis');
                return;
            }

            console.log(`‚úÖ Connexion Realtime: userId=${userId}, threadId=${threadId}, conversationId=${conversationId}, tenantId=${tenantId}`);

            // Ouvrir WebSocket vers OpenAI - le mod√®le d√©tecte automatiquement la langue
            const openaiWs = await this.openOpenAIConnection(threadId);

            // Stocker la connexion
            this.connections.set(clientWs, {
                userId,
                threadId,
                conversationId,
                tenantId,
                assistantThreadId,
                openaiWs,
                connectedAt: new Date(),
            });

            // Proxy bidirectionnel
            this.setupProxy(clientWs, openaiWs, userId, threadId, conversationId);

            // Confirmer la connexion
            clientWs.send(JSON.stringify({
                type: 'connected',
                threadId,
                timestamp: new Date(),
            }));
        } catch (error) {
            console.error(`‚ùå Erreur connexion: ${error.message}`);
            clientWs.close(1011, 'Erreur serveur');
        }
    }

    /**
     * Ouvrir WebSocket vers OpenAI
     * @param threadId - ID du thread
     */
    private async openOpenAIConnection(threadId: string): Promise<WebSocket> {
        const model = process.env.OPENAI_MODEL_REALTIME || 'gpt-realtime-mini';

        const wsUrl = `wss://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}`;
        const ws = new WebSocket(wsUrl, {
            headers: {
                'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
                'OpenAI-Beta': 'realtime=v1',
            },
        });

        return new Promise((resolve, reject) => {
            ws.on('open', () => {
                console.log(`‚úÖ Connexion OpenAI ouverte pour thread: ${threadId}`);

                // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                // REALTIME: Configuration multilingue - le mod√®le d√©tecte automatiquement la langue
                // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                const sessionConfig = buildRealtimeSessionUpdate(defaultProfile);

                // Log la configuration envoy√©e (premiers 200 chars des instructions pour debug)
                console.log(`üì§ Configuration session multilingue envoy√©e √† OpenAI:`);
                console.log(`   Instructions length: ${sessionConfig.session.instructions.length} chars`);
                console.log(`   Instructions preview: ${sessionConfig.session.instructions.substring(0, 200)}...`);

                ws.send(JSON.stringify(sessionConfig));

                // R√©soudre imm√©diatement pour continuer
                resolve(ws);
            });

            ws.on('error', (error) => {
                console.error(`‚ùå Erreur OpenAI WebSocket: ${error.message}`);
                reject(error);
            });
        });
    }


    /**
     * Convertir PCM16 en WAV
     */
    private pcm16ToWav(pcm: Buffer, sampleRate = 24000, channels = 1): Buffer {
        const byteRate = sampleRate * channels * 2;
        const blockAlign = channels * 2;
        const wav = Buffer.alloc(44 + pcm.length);

        wav.write('RIFF', 0);
        wav.writeUInt32LE(36 + pcm.length, 4);
        wav.write('WAVE', 8);
        wav.write('fmt ', 12);
        wav.writeUInt32LE(16, 16); // PCM
        wav.writeUInt16LE(1, 20);  // PCM
        wav.writeUInt16LE(channels, 22);
        wav.writeUInt32LE(sampleRate, 24);
        wav.writeUInt32LE(byteRate, 28);
        wav.writeUInt16LE(blockAlign, 32);
        wav.writeUInt16LE(16, 34); // bits per sample
        wav.write('data', 36);
        wav.writeUInt32LE(pcm.length, 40);
        pcm.copy(wav, 44);
        return wav;
    }

    /**
     * Configurer le proxy bidirectionnel
     */
    private setupProxy(
        clientWs: WebSocket,
        openaiWs: WebSocket,
        userId: string,
        threadId: string,
        conversationId?: string,
    ) {
        // √âtat STT pour cette connexion
        const stateKey = `${userId}_${threadId}`;
        if (!this.userSTTStates.has(stateKey)) {
            this.userSTTStates.set(stateKey, {
                pcmChunks: [],
                sampleRate: 24000,
                hasReceivedTranscription: false,
                accumulatedTranscript: '',
                responseGenerationStarted: false,
            });
        }
        const sttState = this.userSTTStates.get(stateKey)!;

        // Realtime audio activ√© (streaming natif selon doc officielle)
        const realtimeGenEnabled = true; // ‚úÖ Active response.create et audio streaming

        // Etats de tour
        let aiSpeaking = false;
        let awaitingResponse = false;
        let pendingResponseAfterCommit = false; // Flag pour cr√©er une r√©ponse apr√®s l'item
        let hasRetriedThisTurn = false; // Flag pour √©viter les retries infinis
        const textByResp = new Map<string, string>();
        const audioTranscriptByResp = new Map<string, string>(); // Pour audio_transcript
        const responseStartTimes = new Map<string, string>(); // Timestamp de d√©but de chaque r√©ponse
        const userSpeechStartTimes = new Map<string, string>(); // Timestamp de d√©but de chaque entr√©e utilisateur
        
        // Store pour function_calls
        type FuncCallAcc = { name?: string; args: string; toolCallId?: string };
        const funcCalls = new Map<string, FuncCallAcc>(); // key = item_id
        
        // Flags pour √©viter double voix
        let seenRealtimeAudio = false;
        let sentRestTTS = false;

        // D√©tection micro muet pour √©conomie
        // ‚ö†Ô∏è D√âSACTIV√â pour garantir l'audio : garder ["audio","text"] en continu
        let microMuetTimer: NodeJS.Timeout | null = null;
        let audioMode = true; // true = audio+text, false = text only
        const MICRO_MUET_DELAY = 10000; // 10 secondes d'inactivit√© = text only
        const MICRO_MUET_ENABLED = false; // üëà D√âSACTIV√â pour √©viter l'interruption audio

        // Fonction pour basculer entre audio+text et text only
        const updateModalities = async (enableAudio: boolean) => {
            if (enableAudio === audioMode) return; // Pas de changement

            audioMode = enableAudio;
            const modes = enableAudio ? ['audio', 'text'] : ['text'];

            console.log(`üîÄ Bascule modalities: ${modes.join(', ')}`);

            try {
                openaiWs.send(JSON.stringify({
                    type: 'session.update',
                    session: {
                        modalities: modes,
                    }
                }));
            } catch (e) {
                console.error('‚ùå Erreur update modalities:', e);
            }
        };

        // Annuler le timer de micro muet
        const cancelMicroMuetTimer = () => {
            if (microMuetTimer) {
                clearTimeout(microMuetTimer);
                microMuetTimer = null;
                console.log('üîÑ Timer micro muet annul√© (utilisateur actif)');
            }
        };

        // D√©marrer le timer de micro muet
        const startMicroMuetTimer = () => {
            if (!MICRO_MUET_ENABLED) return; // üëà D√©sactiv√©
            
            cancelMicroMuetTimer();

            microMuetTimer = setTimeout(() => {
                if (!aiSpeaking && !awaitingResponse && MICRO_MUET_ENABLED) {
                    console.log('üîá Micro muet d√©tect√© ‚Üí bascule text-only (√©conomie)');
                    updateModalities(false); // Text only
                }
            }, MICRO_MUET_DELAY);
        };

        // Client ‚Üí OpenAI
        clientWs.on('message', async (data: Buffer) => {
            try {
                if (openaiWs.readyState === WebSocket.OPEN) {
                    const message = JSON.parse(data.toString());
                    
                    // Whitelist stricte des types autoris√©s vers OpenAI
                    const allowedToOpenAI = new Set([
                        'session.update',
                        'transcription_session.update',
                        'input_audio_buffer.append',
                        'input_audio_buffer.commit',
                        'input_audio_buffer.clear',
                        'conversation.item.create',
                        'conversation.item.truncate',
                        'conversation.item.delete',
                        'conversation.item.retrieve',
                        'response.create',
                        'response.cancel',
                    ]);
                    
                    if (!allowedToOpenAI.has(message.type)) {
                        console.log(`‚ö†Ô∏è Message ignor√© (non autoris√©): ${message.type}`);
                        return; // Ne pas forwarder les messages non autoris√©s
                    }
                    
                    console.log(`üì§ Client ‚Üí OpenAI: ${message.type || 'unknown'}`);

                    // Log d√©taill√© du message envoy√© (sauf pour l'audio qui est trop long)
                    if (message.type !== 'input_audio_buffer.append') {
                        console.log(`   Message envoy√©: ${JSON.stringify(message)}`);
                    }

                    if (message.type === 'input_audio_buffer.append' && message.audio) {
                        // Ne PAS dropper les append - laisser le VAD serveur g√©rer
                        // Le filtrage automatique est g√©r√© par le VAD serveur
                        // Accumuler les chunks PCM
                        const pcm = Buffer.from(message.audio, 'base64');
                        sttState.pcmChunks.push(pcm);

                        const audioMessage = {
                            type: 'input_audio_buffer.append',
                            audio: message.audio,
                        };

                        console.log(`üé§ Formatant audio pour OpenAI: ${message.audio.substring(0, 50)}...`);
                        openaiWs.send(JSON.stringify(audioMessage));

                    } else if (message.type === 'input_audio_buffer.commit') {
                        // En VAD serveur + cr√©ation auto, on laisse le serveur committer
                        const useServerVAD = true; // Align√© avec session.update
                        if (!useServerVAD) {
                            console.log('üì§ Client ‚Üí OpenAI: input_audio_buffer.commit');
                            openaiWs.send(JSON.stringify({ type: 'input_audio_buffer.commit' }));
                            awaitingResponse = true;
                            pendingResponseAfterCommit = realtimeGenEnabled;
                            sttState.hasReceivedTranscription = false;
                        } else {
                            console.log('‚è≠Ô∏è Commit client ignor√© (server_vad actif)');
                        }

                    } else if (message.type === 'input_audio_buffer.speech_started') {
                        console.log('üé§ Speech started - cancel response (seulement si IA en parle)');
                        // Ne pas envoyer response.cancel ici, sera fait dans openaiWs.on('message')
                    } else {
                        openaiWs.send(JSON.stringify(message));
                    }
                } else {
                    clientWs.send(JSON.stringify({
                        type: 'error',
                        message: 'Connexion OpenAI ferm√©e',
                    }));
                }
            } catch (error) {
                console.error(`‚ùå Erreur traitement message: ${error.message}`);
            }
        });

        // OpenAI ‚Üí Client
        openaiWs.on('message', async (data: Buffer) => {
            try {
                if (clientWs.readyState === WebSocket.OPEN) {
                    const message = JSON.parse(data.toString());

                    // Filtrer les logs d'entr√©e pour response.* si Realtime d√©sactiv√©
                    const isRealtimeResponseEvent =
                        typeof message.type === 'string' && message.type.startsWith('response.');

                    if (realtimeGenEnabled || !isRealtimeResponseEvent) {
                        console.log(`üì• OpenAI ‚Üí Client: ${message.type || 'unknown'}`);
                    }

                    // V√©rifier le mod√®le utilis√©
                    if (message.type === 'session.updated') {
                        console.log('üîé Session model:', message.session?.model);
                        console.log('üîé Session modals:', message.session?.modalities);
                    }

                    // Log d√©taill√© du message (seulement si ce n'est pas un event response.* ou si Realtime enabled)
                    if (realtimeGenEnabled || !isRealtimeResponseEvent) {
                        if (message.item_id) {
                            console.log(`   item_id: ${message.item_id}`);
                        }
                        if (message.event_id) {
                            console.log(`   event_id: ${message.event_id}`);
                        }
                        if (message.hasAudio !== undefined) {
                            console.log(`   hasAudio: ${message.hasAudio}`);
                        }
                        if (message.hasTranscript !== undefined) {
                            console.log(`   hasTranscript: ${message.hasTranscript}`);
                        }
                        if (message.hasDelta !== undefined) {
                            console.log(`   hasDelta: ${message.hasDelta}`);
                        }
                        if (message.delta) {
                            console.log(`   delta: ${JSON.stringify(message.delta).substring(0, 100)}`);
                        }
                        if (message.transcript) {
                            console.log(`   transcript: ${message.transcript}`);
                        }
                        if (message.text) {
                            console.log(`   text: ${message.text}`);
                        }
                        if (message.audio) {
                            console.log(`   audio length: ${message.audio.length} bytes`);
                        }
                    }

                    if (message.response && realtimeGenEnabled) {
                        console.log(`   response: ${JSON.stringify(message.response).substring(0, 200)}`);
                        // Afficher les erreurs compl√®tes
                        if (message.response.status === 'failed' || message.response.status === 'error') {
                            console.log(`‚ùå ERREUR OPENAI: ${JSON.stringify(message.response.status_details || message.response.error)}`);
                        }
                    }
                    if (message.error) {
                        // Ne pas logger les erreurs response_cancel_not_active
                        if (message.error.code !== 'response_cancel_not_active') {
                            if (realtimeGenEnabled) {
                                console.log(`‚ùå ERREUR MESSAGE: ${JSON.stringify(message.error)}`);
                            }
                        }
                    }

                    // Assembler le texte assistant via deltas (texte brut)
                    if (message.type === 'response.output_text.delta') {
                        const id = message.response_id;
                        textByResp.set(id, (textByResp.get(id) ?? '') + (message.delta ?? ''));
                    }
                    if (message.type === 'response.output_text.done') {
                        const id = message.response_id;
                        const full = textByResp.get(id) ?? '';
                        textByResp.delete(id);
                        
                        // NOTE: La journalisation est maintenant g√©r√©e par le FRONT.
                        // Le front doit POSTer vers /api/v1/assistants/thread/messages apr√®s response.completed.
                        // Voir FRONT_REALTIME_INTEGRATION.md
                        
                        clientWs.send(JSON.stringify({
                            type: 'assistant.text',
                            text: full,
                            responseId: id,
                            timestamp: new Date().toISOString()
                        }));
                    }

                    // Assembler la transcription audio en temps r√©el
                    if (message.type === 'response.audio_transcript.delta' || message.type === 'response.output_audio_transcript.delta') {
                        const id = message.response_id;
                        const current = audioTranscriptByResp.get(id) ?? '';
                        const updated = current + (message.delta ?? '');
                        audioTranscriptByResp.set(id, updated);
                        console.log(`üìù Transcript audio delta: "${message.delta}"`);
                    }
                    if (message.type === 'response.audio_transcript.done' || message.type === 'response.output_audio_transcript.done') {
                        const id = message.response_id;
                        const full = audioTranscriptByResp.get(id) ?? '';
                        const startTime = responseStartTimes.get(id); // R√©cup√©rer le timestamp de d√©but
                        audioTranscriptByResp.delete(id);
                        console.log(`‚úÖ Transcript audio complet: "${full}"`);
                        
                        // Journaliser dans le thread si conversationId disponible
                        const connection = this.connections.get(clientWs);
                        if (connection?.conversationId && full.trim().length > 0) {
                            // Journaliser en arri√®re-plan (non bloquant)
                            this.journalizeMessage(connection.conversationId, 'assistant', full).catch((err) => {
                                console.warn(`‚ö†Ô∏è Erreur journalisation transcript assistant: ${err.message}`);
                            });
                        }
                        
                        // Envoyer le texte transcrit au client avec le timestamp de d√©but
                        // Param√®tres: ws, text, responseId?, source?, timestamp?
                        forwardAssistantText(clientWs, full, id, undefined, startTime);
                        // Nettoyer le timestamp
                        if (startTime) {
                            responseStartTimes.delete(id);
                        }
                    }

                    // STREAMING AUDIO Realtime (selon doc officielle)
                    if (message.type === 'response.output_audio.delta' || message.type === 'response.audio.delta') {
                        // message.audio est base64 PCM16
                        if (message.audio) {
                            seenRealtimeAudio = true; // ‚úÖ Marquer qu'on a vu l'audio Realtime
                            console.log(`üîä Audio delta re√ßu (${message.audio.length} bytes)`);
                            try {
                                forwardAssistantAudioDelta(clientWs, message.audio, message.response_id);
                                console.log(`‚úÖ √âv√©nement assistant.audio.delta envoy√© au client`);
                            } catch (error) {
                                console.error(`‚ùå Erreur envoi audio delta:`, error);
                            }
                        } else {
                            console.warn(`‚ö†Ô∏è Audio delta sans audio data`);
                        }
                    }

                    if (message.type === 'response.output_audio.done' || message.type === 'response.audio.done') {
                        console.log('‚úÖ Audio streaming termin√©');
                        clientWs.send(JSON.stringify({
                            type: 'assistant.audio.done',
                            responseId: message.response_id,
                            timestamp: new Date().toISOString()
                        }));
                    }

                    // Si OpenAI commit (VAD serveur)
                    if (message.type === 'input_audio_buffer.committed') {
                        sttState.hasReceivedTranscription = false;
                        awaitingResponse = true;
                        // Le VAD serveur va cr√©er la r√©ponse automatiquement (create_response: true)
                    }

                    // Log conversation item (pour debug)
                    if (message.type === 'conversation.item.created') {
                        console.log('üí¨ Item utilisateur cr√©√© - Le VAD serveur va g√©n√©rer la r√©ponse automatiquement');
                        // NE PAS cr√©er response.create manuellement
                        // Le serveur avec create_response:true le fait automatiquement
                    }

                    // Gestion des tool calls (fonctions) - FLUX "function_call" (Realtime standard)
                    // Handler 1: Nouveau function_call d√©tect√©
                    if (message.type === 'response.output_item.added' && message.item?.type === 'function_call') {
                        const { id: itemId, name, call_id: toolCallId } = message.item;
                        console.log(`üß∞ function_call ajout√©: ${name}, itemId: ${itemId}, toolCallId: ${toolCallId}`);
                        funcCalls.set(itemId, { name, args: '', toolCallId });
                    }
                    
                    // Handler 2: Accumuler les arguments JSON
                    if (message.type === 'response.function_call_arguments.delta') {
                        const itemId = message.item_id;
                        const acc = funcCalls.get(itemId) || { args: '' };
                        acc.args += (message.delta || '');
                        funcCalls.set(itemId, acc);
                    }
                    
                    // Handler 3: Ex√©cuter le function_call quand termin√©
                    if (message.type === 'response.function_call_arguments.done') {
                        const itemId = message.item_id;
                        const acc = funcCalls.get(itemId);
                        
                        if (!acc) {
                            console.warn('‚ö†Ô∏è function_call termin√© sans accumulateur');
                        } else {
                            const { name, args } = acc;
                            console.log(`‚úÖ function_call termin√©: ${name}, args: ${args.substring(0, 100)}...`);
                            
                            // Ex√©cuter le tool via le registre
                            (async () => {
                                try {
                                    // Ex√©cuter le tool via le registre
                                    let output: any = { error: 'unsupported_function' };
                                    
                                    try {
                                        const toolArgs = JSON.parse(args || '{}');
                                        output = await executeTool(name, toolArgs, { userId });
                                        
                                        // Envoyer les citations pour web_search et web_open
                                        if (name === 'web_search' && process.env.WEB_SEARCH_ENABLED === 'true' && output.results) {
                                            forwardAssistantCitations(clientWs, output.results);
                                        } else if (name === 'web_open' && process.env.WEB_SEARCH_ENABLED === 'true' && output) {
                                            forwardAssistantCitations(clientWs, [output]);
                                        }
                                    } catch (toolError) {
                                        console.error(`‚ùå Erreur ex√©cution tool ${name}:`, toolError);
                                        output = { error: toolError.message || 'tool_execution_error' };
                                    }
                                    
                                    // IMPORTANT: Utiliser toolCallId (pas itemId) pour le call_id
                                    const toolCallId = acc.toolCallId || itemId; // Fallback si manquant
                                    
                                    openaiWs.send(JSON.stringify({
                                        type: 'conversation.item.create',
                                        item: {
                                            type: 'function_call_output',
                                            call_id: toolCallId, // ‚úÖ Le tool call ID (pas l'item ID)
                                            output: JSON.stringify(output),
                                        },
                                    }));
                                    
                                    console.log(`‚úÖ function_call_output item cr√©√© pour ${name} (call_id: ${toolCallId})`);
                                    
                                    // Demander explicitement au mod√®le de continuer avec audio forc√©
                                    openaiWs.send(JSON.stringify({
                                        type: 'response.create',
                                        response: { modalities: ['audio'] } // üëà FORCER L'AUDIO
                                    }));
                                    
                                    console.log(`‚úÖ response.create envoy√© pour continuer`);
                                } catch (e: any) {
                                    console.error(`‚ùå Erreur function_call handler:`, e);
                                    
                                    // En cas d'erreur, utiliser le toolCallId correct
                                    const toolCallId = acc.toolCallId || itemId;
                                    
                                    openaiWs.send(JSON.stringify({
                                        type: 'conversation.item.create',
                                        item: {
                                            type: 'function_call_output',
                                            call_id: toolCallId, // ‚úÖ Le tool call ID correct
                                            output: JSON.stringify({ error: e?.message || 'tool_error' }),
                                        },
                                    }));
                                    
                                    // Demander au mod√®le de continuer m√™me en cas d'erreur
                                    openaiWs.send(JSON.stringify({
                                        type: 'response.create',
                                        response: { modalities: ['audio'] } // üëà FORCER L'AUDIO
                                    }));
                                } finally {
                                    funcCalls.delete(itemId);
                                }
                            })();
                        }
                    }

                    // ‚ùå Bloc "tool_use" supprim√© pour √©viter double ex√©cution
                    // On garde uniquement le flux "function_call" ci-dessus

                    // Barge-in: si l'utilisateur parle pendant que l'IA est en train de parler, annuler
                    if (message.type === 'response.created') {
                        // ‚õî Si on n'utilise pas Realtime, on ignore compl√®tement cet event
                        if (!realtimeGenEnabled) {
                            return; // Ne pas traiter cet event du tout
                        }

                        // Enregistrer le timestamp de d√©but de cette r√©ponse
                        const responseId = message.response?.id || '';
                        if (responseId) {
                            responseStartTimes.set(responseId, new Date().toISOString());
                            console.log(`‚è±Ô∏è Timestamp d√©but r√©ponse ${responseId}: ${responseStartTimes.get(responseId)}`);
                        }

                        const respModalities = message.response?.modalities || [];
                        aiSpeaking = Array.isArray(respModalities) && respModalities.includes('audio');
                        console.log(`‚úÖ R√©ponse cr√©√©e, modalities: ${JSON.stringify(respModalities)}, aiSpeaking: ${aiSpeaking}`);

                        // R√©activer l'audio si d√©sactiv√© (pour entendre l'IA)
                        if (!audioMode && aiSpeaking) {
                            console.log('üîä R√©ponse audio ‚Üí r√©activation audio imm√©diate');
                            updateModalities(true);
                            cancelMicroMuetTimer();
                        }
                    }

                    if (message.type === 'input_audio_buffer.speech_started') {
                        // Enregistrer le timestamp de d√©but de la parole utilisateur
                        const itemId = message.item?.id || '';
                        if (itemId) {
                            userSpeechStartTimes.set(itemId, new Date().toISOString());
                            console.log(`‚è±Ô∏è Timestamp d√©but parole utilisateur ${itemId}: ${userSpeechStartTimes.get(itemId)}`);
                        }
                        
                        // nouveau tour utilisateur ‚Üí nettoie le buffer
                        sttState.pcmChunks = [];
                        sttState.hasReceivedTranscription = false;
                        sttState.accumulatedTranscript = '';
                        sttState.responseGenerationStarted = false;
                        
                        // Barge-in assoupli : attendre 200ms avant d'annuler pour √©viter les faux positifs
                        if (aiSpeaking) {
                            setTimeout(() => {
                                if (aiSpeaking && clientWs.readyState === WebSocket.OPEN) {
                            openaiWs.send(JSON.stringify({ type: 'response.cancel' }));
                            aiSpeaking = false;
                                    console.log('üõë Barge-in: response.cancel envoy√© √† OpenAI (apr√®s d√©lai)');
                                }
                            }, 200); // 200ms de d√©lai pour confirmer la parole
                        }

                        // R√©activer l'audio si micro muet (√©conomie) - seulement si activ√©
                        if (!audioMode && MICRO_MUET_ENABLED) {
                            console.log('üé§ Utilisateur parle ‚Üí r√©activation audio');
                            updateModalities(true);
                        }
                        cancelMicroMuetTimer(); // Annuler le timer
                    }

                    // Accumuler les deltas de transcription pour r√©ponse rapide
                    if (message.type === 'conversation.item.input_audio_transcription.delta') {
                        const delta = message.delta || '';
                        if (!sttState.accumulatedTranscript) {
                            sttState.accumulatedTranscript = '';
                        }
                        sttState.accumulatedTranscript += delta;
                        
                        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                        // NOTE: Le mod√®le OpenAI Realtime d√©tecte automatiquement la langue
                        // Pas besoin de mettre √† jour la session - le mod√®le g√®re cela nativement
                        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

                        // REST fallback anticip√© (seulement si Realtime d√©sactiv√©)
                        if (!sttState.responseGenerationStarted && sttState.accumulatedTranscript.length > 5 && !realtimeGenEnabled) {
                            sttState.responseGenerationStarted = true;
                            const startTime = Date.now();
                            console.log(`üì§ G√©n√©ration REST anticip√©e (${sttState.accumulatedTranscript.length} chars: "${sttState.accumulatedTranscript}")`);

                            (async () => {
                                try {
                                    const answer = await this.chatService.generateText(sttState.accumulatedTranscript);

                                    const genTime = Date.now() - startTime;
                                    console.log(`‚úÖ R√©ponse REST g√©n√©r√©e en ${genTime}ms: "${answer.substring(0, 50)}..."`);

                                    if (clientWs.readyState === WebSocket.OPEN) {
                                        forwardAssistantText(clientWs, answer, undefined, 'rest');

                                        console.log('‚úÖ Texte envoy√© imm√©diatement');

                                        (async () => {
                                            try {
                                                const ttsStartTime = Date.now();
                                                console.log('üé§ D√©but g√©n√©ration TTS audio...');
                                                const audioBuffer = await generateTTS(answer, process.env.OPENAI_API_KEY || '');
                                                const ttsTime = Date.now() - ttsStartTime;

                                                if (audioBuffer && clientWs.readyState === WebSocket.OPEN) {
                                                    console.log(`‚úÖ Audio TTS g√©n√©r√© en ${ttsTime}ms (${audioBuffer.length} bytes)`);
                                                    try {
                                                        clientWs.send(JSON.stringify({
                                                            type: 'assistant.audio',
                                                            audio: audioBuffer.toString('base64'),
                                                            format: 'pcm16',
                                                            sampleRate: 24000,
                                                            source: 'rest-tts',
                                                            timestamp: new Date().toISOString(),
                                                        }));
                                                        console.log('‚úÖ Audio envoy√© au client');
                                                    } catch (sendError) {
                                                        console.error('‚ùå Erreur envoi audio:', sendError);
                                                    }
                                                } else if (!audioBuffer) {
                                                    console.warn('‚ö†Ô∏è Pas d\'audio g√©n√©r√© (TTS failed)');
                                                }
                                            } catch (ttsError) {
                                                console.error('‚ùå Erreur g√©n√©ration TTS:', ttsError);
                                            }
                                        })();
                                    }

                                    awaitingResponse = false;
                                    aiSpeaking = false;
                                } catch (e) {
                                    console.error('‚ùå REST generation error:', e);
                                }
                            })();
                        }
                    }

                    // D√©tecter les √©v√©nements de transcription utilisateur (completion)
                    if (message.type === 'conversation.item.input_audio_transcription.completed') {
                        console.log(`üîç √âV√âNEMENT TRANSCRIPTION COMPLET√â: ${message.type}`);
                        sttState.hasReceivedTranscription = true;

                        const text = message.transcript || message.text || '';
                        if (text) {
                            console.log(`üì§ Transcription Realtime re√ßue: "${text.substring(0, 50)}..."`);
                            
                            // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                            // NOTE: Le mod√®le OpenAI Realtime d√©tecte automatiquement la langue
                            // Pas besoin de mettre √† jour la session - le mod√®le g√®re cela nativement
                            // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                            
                            // R√©cup√©rer le timestamp de d√©but de cette transcription
                            const itemId = message.item_id || '';
                            const startTime = itemId ? userSpeechStartTimes.get(itemId) : undefined;
                            
                            // M√©moriser la transcription pour le fallback REST
                            sttState.lastTranscript = text;

                            // NOTE: La journalisation est maintenant g√©r√©e par le FRONT.
                            // Le front doit POSTer vers /api/v1/assistants/thread/messages apr√®s transcription.
                            // Voir FRONT_REALTIME_INTEGRATION.md

                            // Envoyer au client pour affichage UI avec le bon timestamp
                            forwardUserTranscript(clientWs, text, startTime);
                            
                            // Nettoyer le timestamp
                            if (itemId && startTime) {
                                userSpeechStartTimes.delete(itemId);
                            }

                            // REST fallback (seulement si Realtime d√©sactiv√©)
                            if (!sttState.responseGenerationStarted && !realtimeGenEnabled) {
                                sttState.responseGenerationStarted = true;
                                const startTime = Date.now();
                                console.log(`üì§ G√©n√©ration REST d√©clench√©e apr√®s transcription compl√®te`);
                                (async () => {
                                    try {
                                        const answer = await this.chatService.generateText(text);

                                        const genTime = Date.now() - startTime;
                                        console.log(`‚úÖ R√©ponse REST g√©n√©r√©e en ${genTime}ms: "${answer.substring(0, 50)}..."`);

                                        if (clientWs.readyState === WebSocket.OPEN) {
                                            forwardAssistantText(clientWs, answer, undefined, 'rest');

                                            console.log('‚úÖ Texte envoy√© imm√©diatement');

                                            (async () => {
                                                try {
                                                    const ttsStartTime = Date.now();
                                                    console.log('üé§ D√©but g√©n√©ration TTS audio...');
                                                    const audioBuffer = await generateTTS(answer, process.env.OPENAI_API_KEY || '');
                                                    const ttsTime = Date.now() - ttsStartTime;

                                                    if (audioBuffer && clientWs.readyState === WebSocket.OPEN) {
                                                        console.log(`‚úÖ Audio TTS g√©n√©r√© en ${ttsTime}ms (${audioBuffer.length} bytes)`);
                                                        try {
                                                            clientWs.send(JSON.stringify({
                                                                type: 'assistant.audio',
                                                                audio: audioBuffer.toString('base64'),
                                                                format: 'pcm16',
                                                                sampleRate: 24000,
                                                                source: 'rest-tts',
                                                                timestamp: new Date().toISOString(),
                                                            }));
                                                            console.log('‚úÖ Audio envoy√© au client');
                                                        } catch (sendError) {
                                                            console.error('‚ùå Erreur envoi audio:', sendError);
                                                        }
                                                    } else if (!audioBuffer) {
                                                        console.warn('‚ö†Ô∏è Pas d\'audio g√©n√©r√© (TTS failed)');
                                                    }
                                                } catch (ttsError) {
                                                    console.error('‚ùå Erreur g√©n√©ration TTS:', ttsError);
                                                }
                                            })();
                                        } else {
                                            console.warn('‚ö†Ô∏è Client d√©connect√©, r√©ponse non envoy√©e');
                                        }

                                        awaitingResponse = false;
                                        aiSpeaking = false;
                                    } catch (e) {
                                        console.error('‚ùå REST generation error:', e);
                                    }
                                })();
                            }
                        }

                        // Reset l'accumulateur
                        sttState.accumulatedTranscript = '';
                        sttState.responseGenerationStarted = false;
                    }

                    // D√©tecter si la transcription Realtime a √©chou√©
                    if (message.type === 'conversation.item.input_audio_transcription.failed') {
                        console.log('‚ö†Ô∏è Transcription Realtime √©chou√©e ‚Äì fallback STT pour affichage UI');

                        // D√©clencher imm√©diatement le fallback STT pour affichage UI seulement
                        if (sttState.pcmChunks.length > 0) {
                            try {
                                const pcmBuffer = Buffer.concat(sttState.pcmChunks);
                                const wavBuffer = this.pcm16ToWav(pcmBuffer, sttState.sampleRate);
                                const file = await toFile(wavBuffer, 'user.wav');
                                const transcription = await this.openai.audio.transcriptions.create({
                                    model: process.env.STT_FALLBACK_MODEL || 'gpt-4o-transcribe',
                                    file,
                                });
                                const text = (transcription as any).text || '';
                                console.log(`üì§ Transcription STT (fallback sur erreur): "${text.substring(0, 50)}..."`);

                                // Envoyer au client POUR AFFICHAGE UI UNIQUEMENT
                                // Utiliser un timestamp approximatif pour le fallback
                                forwardUserTranscript(clientWs, text, new Date().toISOString());

                                // ‚ùå NE PAS injecter dans la conversation (d√©j√† fait via audio)
                                // ‚ùå NE PAS cr√©er de response.create
                            } catch (error) {
                                console.error('‚ùå Erreur STT fallback (sur erreur):', error);
                            } finally {
                                sttState.pcmChunks = [];
                            }
                        }
                    }

                    // Fin de r√©ponse: r√©initialiser l'√©tat audio
                    if (message.type === 'response.done') {
                        // Ne plus faire de retry Realtime (g√©n√©ration via REST uniquement)
                        if (realtimeGenEnabled) {
                            console.log('üîö R√©ponse termin√©e, r√©initialisation √©tat');
                        }

                        // Nettoyer les maps de texte/transcript
                        const responseId = message.response?.id;
                        if (responseId) {
                            textByResp.delete(responseId);
                            audioTranscriptByResp.delete(responseId);
                        }

                        sttState.pcmChunks = [];
                        sttState.hasReceivedTranscription = false;
                        sttState.accumulatedTranscript = '';
                        sttState.responseGenerationStarted = false;
                        awaitingResponse = false;
                        aiSpeaking = false;
                        pendingResponseAfterCommit = false; // R√©initialiser le flag
                        hasRetriedThisTurn = false; // Reset pour le prochain tour
                        
                        // R√©initialiser les flags pour √©viter double voix
                        seenRealtimeAudio = false;
                        sentRestTTS = false;

                        // D√©marrer le timer de micro muet (√©conomie apr√®s inactivit√©)
                        startMicroMuetTimer();
                    }

                    // Fallback REST si Realtime √©choue ET aucun audio Realtime n'a d√©marr√©
                    if ((message.type === 'error' || (message.type === 'response.done' && message.response?.status === 'failed')) &&
                        !seenRealtimeAudio && !sentRestTTS) {
                        const error = message.error || message.response?.status_details?.error;
                        if (error && realtimeGenEnabled) {
                            console.warn(`üö® Realtime failed ‚Üí REST fallback: ${error.message || 'unknown error'}`);
                            sentRestTTS = true; // Marquer qu'on a envoy√© un TTS REST

                            // G√©n√©rer avec REST
                            const userPrompt = sttState.lastTranscript || sttState.accumulatedTranscript || 'OK';

                            (async () => {
                                try {
                                    const answer = await this.chatService.generateText(userPrompt);

                                    if (clientWs.readyState === WebSocket.OPEN) {
                                        forwardAssistantText(clientWs, answer, undefined, 'rest-fallback');

                                        // TTS en arri√®re-plan
                                        (async () => {
                                            try {
                                                const audioBuffer = await generateTTS(answer, process.env.OPENAI_API_KEY || '');
                                                if (audioBuffer && clientWs.readyState === WebSocket.OPEN) {
                                                    clientWs.send(JSON.stringify({
                                                        type: 'assistant.audio',
                                                        audio: audioBuffer.toString('base64'),
                                                        format: 'pcm16',
                                                        sampleRate: 24000,
                                                        source: 'rest-tts',
                                                        timestamp: new Date().toISOString(),
                                                    }));
                                                }
                                            } catch (e) {
                                                console.error('‚ùå REST TTS fallback error:', e);
                                            }
                                        })();
                                    }
                                } catch (e) {
                                    console.error('‚ùå REST fallback error:', e);
                                }
                            })();
                        }
                    }

                    // Ne pas relayer les events response.* au front quand Realtime est d√©sactiv√©
                    if (!realtimeGenEnabled && isRealtimeResponseEvent) {
                        return;
                    }

                    // Ne pas relayer les erreurs response_cancel_not_active
                    if (message.type === 'error' && message.error?.code === 'response_cancel_not_active') {
                        return;
                    }
                    
                    // ‚ö†Ô∏è IMPORTANT: Ne jamais bloquer les √©v√©nements Realtime
                    // Les handlers sp√©cialis√©s les traitent et les transforment en events custom
                    // Le blocking des events arrive APR√àS le traitement par les handlers
                    const isRealtimeEvent = typeof message.type === 'string' && message.type.startsWith('response.');
                    const isAudioEvent = message.type === 'response.output_audio.delta' || 
                                         message.type === 'response.audio.delta' ||
                                         message.type === 'response.output_audio.done' ||
                                         message.type === 'response.audio.done' ||
                                         message.type === 'response.audio_transcript.delta' ||
                                         message.type === 'response.output_audio_transcript.delta' ||
                                         message.type === 'response.audio_transcript.done' ||
                                         message.type === 'response.output_audio_transcript.done';
                    
                    // Les handlers audio sont appel√©s AVANT ce bloc
                    // Donc on bloque l'envoi brut SAUF pour les events audio qui doivent passer
                    // Mieux: ne bloquer QUE les √©v√©nements qui sont d√©j√† g√©r√©s par nos handlers custom
                    if (isRealtimeEvent && realtimeGenEnabled && !isAudioEvent) {
                        // On a d√©j√† trait√© l'event dans les handlers ci-dessus (assistant.text, assistant.audio.delta, etc.)
                        // Ne pas envoyer l'event brut √† nouveau
                        return;
                    }

                    // Transf√©rer vers le client (seulement les √©v√©nements non-Realtime ou n√©cessaires)
                    // ‚úÖ BUG FIX: Envoyer JSON string, pas objet brut
                    clientWs.send(JSON.stringify(message));
                }
            } catch (error) {
                console.error(`‚ùå Erreur transfert r√©ponse: ${error.message}`);
            }
        });

        // Keep-alive (ping p√©riodique)
        const pingInterval = setInterval(() => {
            try {
                if (clientWs.readyState === WebSocket.OPEN) {
                    clientWs.ping();
                }
                if (openaiWs.readyState === WebSocket.OPEN) {
                    openaiWs.ping();
                }
            } catch { }
        }, 30000);

        // G√©rer la fermeture
        clientWs.on('close', () => {
            console.log(`üîå Fermeture connexion client: ${userId}`);
            clearInterval(pingInterval);
            cancelMicroMuetTimer(); // Nettoyer le timer

            if (openaiWs.readyState === WebSocket.OPEN) {
                openaiWs.close();
            }

            // Nettoyer l'√©tat STT
            this.userSTTStates.delete(stateKey);

            this.connections.delete(clientWs);
        });

        openaiWs.on('close', () => {
            console.log(`üîå Fermeture connexion OpenAI pour thread: ${threadId}`);
            clearInterval(pingInterval);
            cancelMicroMuetTimer(); // Nettoyer le timer

            if (clientWs.readyState === WebSocket.OPEN) {
                clientWs.close();
            }

            // Nettoyer l'√©tat STT
            this.userSTTStates.delete(stateKey);

            this.connections.delete(clientWs);
        });
    }

    /**
     * Obtenir les stats d'une connexion
     */
    getConnectionStats(userId: string, threadId: string) {
        const connection = Array.from(this.connections.values()).find(
            conn => conn.userId === userId && conn.threadId === threadId
        );

        return connection ? {
            userId,
            threadId,
            connected: true,
            connectedAt: connection.connectedAt,
            duration: Date.now() - connection.connectedAt.getTime(),
        } : null;
    }

    /**
     * Obtenir toutes les connexions actives
     */
    getAllConnections() {
        return Array.from(this.connections.values()).map(conn => ({
            userId: conn.userId,
            threadId: conn.threadId,
            connectedAt: conn.connectedAt,
            duration: Date.now() - conn.connectedAt.getTime(),
        }));
    }

    /**
     * Fermer toutes les connexions
     */
    closeAll() {
        this.connections.forEach((conn, key) => {
            conn.openaiWs.close();
            this.connections.delete(key);
        });

        this.wss.close();
    }
}